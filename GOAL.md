Automation of Systematic Reviews with Large Language Models
 View ORCID ProfileChristian Cao,  View ORCID ProfileRohit Arora,  View ORCID ProfilePaul Cento,  View ORCID ProfileKatherine Manta,  View ORCID ProfileElina Farahani,  View ORCID ProfileMatthew Cecere,  View ORCID ProfileAnabel Selemon, Jason Sang,  View ORCID ProfileLing Xi Gong,  View ORCID ProfileRobert Kloosterman, Scott Jiang,  View ORCID ProfileRichard Saleh,  View ORCID ProfileDenis Margalik,  View ORCID ProfileJames Lin,  View ORCID ProfileJane Jomy,  View ORCID ProfileJerry Xie,  View ORCID ProfileDavid Chen,  View ORCID ProfileJaswanth Gorla,  View ORCID ProfileSylvia Lee,  View ORCID ProfileKelvin Zhang,  View ORCID ProfileHarriet Ware,  View ORCID ProfileMairead Whelan,  View ORCID ProfileBijan Teja,  View ORCID ProfileAlexander A. Leung,  View ORCID ProfileLina Ghosn,  View ORCID ProfileRahul K. Arora, Allen S. Detsky,  View ORCID ProfileMichael Noetel,  View ORCID ProfileDavid B. Emerson,  View ORCID ProfileIsabelle Boutron,  View ORCID ProfileDavid Moher,  View ORCID ProfileGeorge Church,  View ORCID ProfileNiklas Bobrovitz
doi: https://doi.org/10.1101/2025.06.13.25329541
This article is a preprint and has not been peer-reviewed [what does this mean?]. It reports new medical research that has yet to be evaluated and so should not be used to guide clinical practice.
AbstractFull TextInfo/HistoryMetrics Preview PDF
Abstract
Systematic reviews (SRs) inform evidence-based decision making. Yet, they take over a year to complete, are prone to human error, and face challenges with reproducibility; limiting access to timely and reliable information. We developed otto-SR, an end-to-end agentic workflow using large language models (LLMs) to support and automate the SR workflow from initial search to analysis. We found that otto-SR outperformed traditional dual human workflows in SR screening (otto-SR: 96.7% sensitivity, 97.9% specificity; human: 81.7% sensitivity, 98.1% specificity) and data extraction (otto-SR: 93.1% accuracy; human: 79.7% accuracy). Using otto-SR, we reproduced and updated an entire issue of Cochrane reviews (n=12) in two days, representing approximately 12 work-years of traditional systematic review work. Across Cochrane reviews, otto-SR incorrectly excluded a median of 0 studies (IQR 0 to 0.25), and found a median of 2.0 (IQR 1 to 6.5) eligible studies likely missed by the original authors. Meta-analyses revealed that otto-SR generated newly statistically significant findings in 2 reviews and negated significance in 1 review. These findings demonstrate that LLMs can rapidly conduct and update systematic reviews with superhuman performance, laying the foundation for automated, scalable, and reliable evidence synthesis.

1 Introduction
Systematic reviews (SRs) are the foundation of evidence-based decision-making. However, SRs are incredibly resource-intensive, typically taking over 16 months and costing upwards of $100,000 to complete1,2. Delays in completing SRs can have major consequences for evidence-based practice, including failure to adopt effective therapies, or prolonged use of ineffective or harmful treatments initially supported by less rigorous evidence3. While several tools have been developed to accelerate SRs4,5, none are capable of full automation with human-level accuracy. However, large language models (LLMs) offer new avenues to achieve automation with their ability to process and reason about natural language. We previously demonstrated that LLMs can achieve high screening performance6. Other recent work has demonstrated promise for LLMs in data extraction7,8, though these studies rely on self-defined reference standards and evaluate on small datasets.

We introduce an LLM-based workflow (otto-SR) to support automated and human-in-the-loop SR workflows, from initial search to data analysis. Our framework uses GPT-4.1 (OpenAI) for screening articles and o3-mini-high (OpenAI) for data extraction, targeting tasks that typically consume the majority of human researcher time and effort. We evaluate our workflow on these core SR components, article screening and data extraction, with direct comparisons to traditional human workflows and other SR automation tools. To assess real-world utility, we reproduced and updated an entire issue of Cochrane reviews (n=12) using otto-SR, in under two days. otto-SR is designed to work alongside researchers, requiring only a protocol (objectives, eligibility criteria), search results, and defined extraction variables.

2 An agentic workflow for systematic review automation
The gold-standard systematic review workflow begins with a comprehensive search to capture all potentially relevant citations9. These citations undergo abstract and full-text screening by two human reviewers indepen-dently, with disagreements resolved by a third reviewer. The final set of relevant articles then undergo data extraction by two human reviewers independently, again adjudicated by a third reviewer when discrepancies arise. The complete human workflow is illustrated in Figure 1 (top).

Figure 1:
Download figureOpen in new tab
Figure 1:
An automated systematic review workflow using LLMs.
Infographic displaying the end-to-end SR process for humans (grey) and otto-SR (green). Abbreviations: Markdown file (MD)

otto-SR is an end-to-end LLM-based workflow supporting both fully automated and human-in-the-loop systematic reviews. Citations identified from the original search are directly uploaded, in RIS format, to the otto-SR screening agent, which uses GPT-4.1 to screen abstract and full-text articles as a standalone reviewer. The resulting set of included articles is then fed into the otto-SR extraction agent, which performs data extraction with the o3-mini-high model. For full-text screening and data extraction, retrieved PDFs are processed by Gemini 2.0 flash and converted into structured Markdown (MD) files for downstream tasks. An overview of the otto-SR workflow is provided in Figure 1 (bottom).

3 LLMs achieve state-of-the-art SR screening performance
We previously found that GPT4-preview could achieve high screening performance with effective prompting strategies6. Aiming to improve on these findings, we developed a screening agent leveraging GPT-4.1, a model which excels at instruction following10,11, paired with optimized prompting strategies6, to screen articles at abstract and full-text stages. The agent was prompted using the original, unaltered objectives and eligibility criteria from each respective review (Supplementary Notes). Full-text article PDFs were converted into markdown format with the Gemini 2.0 Flash model for full-text screening.

We evaluated the performance of the otto-SR screening agent on the complete original search across five published reviews (n=32,357 citations) covering four Oxford Centre for Evidence-Based Medicine (CEBM) question types: prevalence, diagnostic test accuracy, prognosis, intervention benefits (Extended Data Table 1). Dual human reviewers and Elicit (a commercial LLM-based SR automation software) were evaluated against a random representative sample of records for each review (n=1,767 citations) (Methods). The reference standard for inclusion/exclusion decisions was based on the original authors’ final decisions after full-text screening.

To validate the proficiency of our human reviewers in screening, we conducted a calibration exercise (n=400 citations) where we compared the SR screening performance of our reviewers to the original study authors12, who had independently re-screened the same set of articles. We found that the performance of our human reviewers closely aligned with the original study authors (Our team: 80.2% sensitivity 97.7% specificity vs. original author team: 81.3% sensitivity, 98.1% specificity) providing confidence that our reviewers were reflective of expert-level screening (Extended Data Table 2).

At the abstract screening stage, the otto-SR screening agent achieved the highest sensitivity (weighted sensitivity 96.6% [total range, 94.1-100.0%]) (Fig. 2, Extended Data Table 3). In comparison, Elicit (88.5% [76.9-100%] sensitivity) and dual human reviewers (87.3% [84.1-100%] sensitivity) had lower sensitivity. Dual human reviewers achieved the highest specificity in abstract screening (95.7% [92.5-98.7%] specificity), followed by the otto-SR screening agent (93.9% [83.6-97.7%] specificity) and Elicit (84.2% [65.7-95.9%] specificity).

Figure 2:
Download figureOpen in new tab
Figure 2:
otto-SR screening agent (GPT-4.1) achieves superhuman screening sensitivity and specificity A. Diagram of otto-SR abstract screening agent (left), sensitivity, specificity of otto-SR screening agent, dual human reviewers, and Elicit, for abstract screening evaluated across five reviews (middle). Weighted averages for sensitivity and specificity across comparator groups (right). Error bars indicate 95% confidence intervals. B. diagram of otto-SR full-text screening agent (left), sensitivity, specificity, and accuracy of otto-SR screening agent evaluated across five reviews, and dual human reviewers for full-text screening evaluated across five reviews (middle). Weighted average for sensitivity and specificity across otto-SR (five reviews) and dual human (four reviews) (right). Error bars indicate 95% confidence intervals.

After full-text screening, the otto-SR screening agent maintained the highest sensitivity (96.2% [92.3-100%] sensitivity), while human reviewers had a marked drop in sensitivity (63.3% [44.1-93.8%] sensitivity) (Fig. 2, Extended Data Table 4). This decline in human sensitivity was largely driven by poor performance on screening the “Reinfection” review (44.1% sensitivity, 95.3% specificity), likely due to complex inclusion criteria involving test-negative study designs, multiple interventions, and multiple time-specific outcomes. After removing this outlier review, human reviewers achieved a weighted sensitivity of 81.7% [76.4%-93.8%]. Specificity remained high for both the otto-SR screening agent (96.9% [90.7-98.7%] specificity) and dual human reviewers (98.1% [96.7-100.0%] specificity). Elicit was not included in this comparison as it did not support full-text screening.

Together, these findings suggest that the otto-SR screening agent can capture more relevant studies (true positives) than traditional dual human screening, while maintaining comparable specificity (minimizing false inclusions).

4 LLMs achieve state-of-the-art SR data extraction performance
Given the time-intensive nature of manual data extraction in SRs, we explored if advances in LLM reasoning could provide a path towards automation. To this end, we developed an extraction agent using the OpenAI o3-mini-high model13, selected for its strong scientific reasoning, robust long-context retrieval, and cost. In all cases, the otto-SR extraction agent was prompted with original author-defined variable descriptions. Full-text article PDFs were also converted into markdown format with the Gemini 2.0 Flash model for data extraction. We evaluated the performance of the otto-SR extraction agent and Elicit in data extraction across seven published reviews (n=4,559 data points, 495 studies) (Fig. 3A, Extended Data Table 5). Dual human reviewers were assessed on a randomly sampled subset of articles from each review based on a McNemar test sample size approximation (n=1,453 data points, 156 studies) (Methods). Extracted variables included key descriptive and outcome data used by the original authors for downstream analysis (see Supplementary Notes).

Figure 3:
Download figureOpen in new tab
Figure 3:
otto-SR extraction agent (o3-mini-high) performance on systematic review data extraction. A. Bar graph displaying data extraction accuracy of the otto-SR extraction agent (green) (4,459 data points), Elicit (teal) (4,459 data points), and dual human reviewers (grey) (1,453 data points) across 7 different systematic reviews. Error bars represent 95% confidence intervals. Shading represents pre-(lighter) and post-(darker) human adjudicated correction. B. Dot plot depicting literature-derived human reviewer performance comparison against human reviewers in this study. Dots represent mean value and upper and lower bars represent range. C. Bar graph depicting dual human adjudicator decisions for values marked as incongruent between original review and otto-SR extraction agent, Elicit, and dual human. Blue represents the newly conducted review being correct, while tan represents the original study authors being correct. Error bars represent 95% confidence intervals.

Data extraction accuracy was determined through an LLM-as-a-judge framework to compare AI-or human extracted values against the original author extractions (Methods). However, given the known variability in dual human data extraction accuracy (reported rates: 65.8-85.5%)14–19, original author-extracted values were not treated as a definitive gold standard (Fig. 3B). Instead, we applied a blinded adjudication process to resolve discrepancies between otto-SR extraction and the original authors. A panel of blinded human reviewers compared randomized pairs of responses (otto-SR vs. original author) and selected the most accurate value (see Methods). We used these judgements to construct a corrected gold standard for performance evaluations.

Across all seven reviews, the otto-SR extraction agent achieved an average weighted accuracy of 93.1% (91.1-97.0%), outperforming both dual human reviewers at 79.7% (69.1-91.0%), and Elicit at 74.8% (58.8-83.1%) (Fig. 3A, Extended Data Figure 6). When otto-SR extracted different values to the original authors, the blinded human reviewer panel sided with the otto-SR data extraction agent in 69.3% of cases (Fig. 3C). In contrast, for discrepancies between original authors and our two human extractors or Elicit, the blinded reviewer panel sided with the dual human extractors in 28.1% of cases, and Elicit in 22.4% of cases (Fig. 3C).

In the 6.9% of cases where the otto-SR extraction agent was incorrect, post-hoc analysis revealed that 0.83% (39/4459) of data points were inaccessible to the model (supplementary files or data obtained through data request), 0.67% (30/4459) resulted from parsing errors, and 0.49% (22/4459) were cases where neither the otto-SR data extraction agent nor original author extraction was correct (Extended Data Figure 1).

5 An agentic workflow of LLMs can rapidly reproduce and update reviews
Given the high performance of our screening and extraction agents, we combined them into an agentic workflow, dubbed otto-SR (Fig. 4A). To evaluate the real-world applicability of otto-SR, we conducted a reproducibility assessment of a complete issue of SRs published in the Cochrane Database of Systematic Reviews.

Figure 4:
Download figureOpen in new tab
Figure 4:
Evaluating otto-SR for automation of systematic reviews.
A. Infographic depicting use of otto-SR for systematic review automation in a complete edition of the Cochrane Database of Systematic Reviews (n = 12). B. Forest plots depicting differences between otto-SR (green), original Cochrane study authors (purple), and corrected standard (gold). Each row is representative of meta-analyzed estimates derived in a systematic review. Error bars represent 95% confidence interval, MD = Mean Difference, OR = Odds Ratio, RR = Risk Ratio, SMD = Standardized Mean Difference. The matched comparison (left) shows estimates derived from articles only included in the original Cochrane reviews. The expanded comparison (middle) displays estimates derived from additional articles identified by otto-SR falling within the original search dates. The update plot (right) displays estimates derived from all articles found by otto-SR in a May 8 2025 search. *otto-SR discovered a new treatment group, mixed oral / enteral nutrition, which was not found in the original Cochrane review, consequently no matched analysis was conducted. **workplace citations were provided by original study authors due to challenges with the electronic search, consequently no updated search was performed.

We randomly selected the April 2024 issue of the Cochrane Database (Extended Data Table 7). Of the 14 reviews in this issue, one review was excluded due to a lack of publicly available data, and a second review was excluded due to the absence of a reproducible search strategy (Extended Data Table 7). For the 12 remaining reviews, we reproduced their reported search strategies, updating searches to May 8, 2025, and identified 146,276 citations. These citations were deduplicated and then screened at both the abstract and full-text stages using the otto-SR screening agent with original Cochrane review eligibility criteria (Supplementary Notes).

To ensure a focused and interpretable comparison, we diverged from Cochrane methodology in one key respect. Cochrane reviews typically include all studies, regardless of whether they report the review’s primary outcome, to allow for all comparisons based on the available data (e.g., all intervention and outcome combinations). In contrast, we focused our analysis to reproduce each review’s predefined primary outcome. This constraint provided a clearer distinction for study eligibility.

The otto-SR screening agent correctly identified all included studies (n=64) across the 12 Cochrane reviews. Citations passing screening then had primary outcome data extracted using the otto-SR extraction agent and original Cochrane study variable definitions (Supplementary Notes). otto-SR extraction results with missing primary outcome values, duplicate studies, or missing intervention-comparator groups were programmatically excluded (Methods). After this process, otto-SR incorrectly excluded a median of 0 articles (IQR 0 to 0.25) (Extended Data Table 8). Incorrect exclusions were due to LLM-inaccessible supplementary data (n=2), or a failure to extract reported outcome values when present (n=2).

After filtering our results to align with the original search cutoffs, we identified 54 additional eligible studies through otto-SR (median 2, IQR: 1 to 6.25 per review) that were likely missed in the original Cochrane reviews (Methods). otto-SR also incorrectly included 10 false positive articles after human review; however 9/10 may have contained relevant data available through additional author correspondence. Updating the search to May 8, 2025 identified another 14 new eligible studies (total n = 64, median 2.5, IQR 1 to 7.25 per review) (Extended Data Table 8). The updated search identified two additional false positive studies, one of which may have contained relevant data.

Extracted data was subsequently meta-analyzed using the same statistical methods as the original reviews, across three comparisons: (1) ‘Matched’ where otto-SR was restricted to the same set of articles as included in the original Cochrane analysis. (2) ‘Expanded’ which included all eligible studies identified by otto-SR, filtered to the original search cutoff date. (3) ‘Update’ which evaluated all articles with an updated May 8, 2025 search cutoff.

Given potential data extraction errors by original Cochrane authors and otto-SR, we derived corrected values for each comparison through dual human review. This also included removal of false positive articles and addition of false negative articles. For each review, we also generated corresponding Cochrane meta-analyses using the original author-extracted data. All original Cochrane data, otto-SR extracted data, and corrected data (including notes) are provided in Supplementary Data 1.

In the ‘Matched’ comparison group, otto-SR produced meta-analyzed effect estimates which had overlapping 95% CIs with both the original Cochrane data and corrected datasets across all reviews (Fig. 4B, left; Extended Data Table 9). In the ‘Expanded’ analysis, two reviews (nutrition, depression) yielded new statistically significant effect estimates (Fig. 4B, middle), while the estimate from one review (alcohol) lost statistical significance compared to the original Cochrane estimates (Fig. 4B, middle). These trends were consistent in the corrected ‘Expanded’, otto-SR ‘updated’, and the corrected ‘Update’ analyses (Fig. 4B, right).

One illustrative example comes from the nutrition review, where otto-SR identified 5 additional studies. This led to the new finding that preoperative immune-enhancing supplementation before gastric surgery is associated with a one-day reduction in mean hospital stay compared to usual care (otto-SR: MD-1.20 [95% CI-2.28 to-0.11], 9 studies; Cochrane: MD-0.19 [-1.44 to 1.07], 4 studies). Detailed effect estimates and 95% CIs are provided for all groups and comparisons are provided in Extended Data Table 9.

6 Discussion
Systematic review workflows are often hindered by the time-and labor-intensive demands of screening and data extraction. In this study, we demonstrate that otto-SR, an end-to-end SR automation pipeline, powered by GPT 4.1 and o3-mini-high, can accelerate these steps without compromising performance.

Our findings highlight several opportunities for LLMs to be implemented in systematic reviews. First, workflows like otto-SR can be used to update existing systematic reviews by leveraging their original published protocols. This provides a unique advantage: it enables direct comparison of screening and extraction results against the original review, facilitating validation and assessments of reproducibility. Second, the ability to rapidly process articles opens the door to truly living systematic reviews–where updates could be performed monthly, weekly, or even daily–ensuring constant access to the most current evidence. Third, otto-SR may be used to generate de novo reviews, provided that researchers develop clear, detailed protocols akin to those pre-registered in PROSPERO. In all cases, structured and clear methodology is essential for ensuring interpretability, reproducibility, and high-quality automation.

Our Cochrane reproducibility assessment highlighted common reproducibility challenges. All 12 reviews had issues with search reproducibility and 2 reviews lacked methodological clarity. These findings align with prior work by Rethlefsen et al20, who found that only 1% of reviews have a fully reproducible search strategy. Previous studies have also shown that reproducibility failures can occur at every stage of the SR process20–24. To address these challenges, we suggest that published SRs include the following: (i) the complete search strategy; (ii) raw search files (e.g., RIS file); (iii) raw data extraction outputs with data dictionaries; (iv) list of data procured via author correspondence; and (v) the complete code used for analyses. Current reporting guidelines for systematic reviews (PRISMA) endorse most, but not all of these points25. While Cochrane reviews routinely provide such materials, most other SRs do not, limiting reproducibility and external validation.

The performance of LLMs in conducting evidence synthesis, as demonstrated in this study, also highlights an opportunity to reconsider how scientific content is published. While most research is written for human readers, the rise of LLM-supported evidence curation highlights the value of making studies machine-readable. Using structured formats like markdown or html (as now offered by Arxiv), and providing raw numerical data from figures could support this new paradigm.

Our work has several limitations. First, although our analysis was validated across a wide range of SRs, further research is needed to assess the generalizability to other clinical questions and qualitative reviews. Second, our LLM parser may have incorrectly extracted information from PDF articles. In future studies, enhanced vision capabilities of new models may better support screening/extraction efforts. Thirdly, our workflow was limited to the main text of articles and did not extract data from supplementary tables or figures. While we could have manually incorporated these materials, we intentionally avoided human intervention to test the end-to-end capabilities of our automated workflow. Fourth, due to the vast number of data points (and their unstandardized nature), we employed an LLM-as-a-judge framework to assess data extraction accuracy. Although this approach has been previously validated26,27, LLM judgements may still introduce occasional errors. Fifth, we encountered instances where the original author’s decisions for data extraction appeared inaccurate. However, we addressed this by performing randomized and blinded adjudication, adopting best practices seen in radiology, ophthalmology, and clinical trials28–31. This approach represented a methodological improvement over prior work that relied on self-created ground truths7. Sixth, the GPT-4.1 knowledge cut-off was updated to June 2024. While our screening and data extraction benchmarks primarily involved closed-source data, it’s possible that the April 2024 Cochrane reviews were included in the model’s pretraining corpus. This may have introduced minor inclusion bias favoring studies already included in the original Cochrane review; though it would not affect newly identified studies. Finally, with respect to the Cochrane reproducibility assessment, it’s possible that content specific experts would make different article inclusion and data extraction decisions. However, we closely adhered to each review’s protocol, contacted study authors to clarify methodological uncertainties, and documented discrepancies.

7 Conclusion
In conclusion, our study marks a major advancement in the development of SR automation tools using LLMs. The immediate applications of this include: rapid updates and truly ‘living’ reviews, mass assessments of reproducibility across the SR literature, and faster de novo reviews. Future research should focus on developing comprehensive and complete benchmarks of SRs to better support and refine automation efforts. We also encourage research into the capabilities of LLMs for other SR workflow tasks, such as search term generation and risk of bias assessment. The implementation of fully autonomous SRs could accelerate the synthesis of up-to-date evidence, save thousands of hours of manual work, and provide significant benefits in medicine and other fields.

8 Methods
8.1 Article Screening Datasets
To identify putative screening datasets, we leveraged the previously published BenchSR database of published SRs6. In brief, this consisted of 10 distinct SRs spanning nine unique clinical domains and contained study information (review objectives, inclusion/exclusion criteria) and the complete set of labeled ‘included’ and ‘excluded’ citations from the original search of each SR.

From the BenchSR database, we performed stratified random sampling across the four Oxford CEBM review questions, selecting SRs for each type. Our sample included various datasets: the SeroTracker dataset32 for reviews of prevalence (calibration set adapted from Perlman-Arrow et al.12), the Reinfection33 and PA-Outcomes34 datasets for reviews of intervention benefits, the PA-Testing35 dataset for reviews of diagnostic test accuracy, and SVCF36 dataset for reviews of prognosis (Extended Data Table 1).

8.2 LLM Screening Methodology
We developed a novel LLM-based screening system adapted from our previously validated ScreenPrompt approach6. Our LLM based screening agent uses the GPT-4.1 model with a 32,768-token output limit and default parameters (temperature=1, frequency_penalty=0, presence_penalty=0, top_p=1).

For full-text screening, we implemented a PDF parsing pipeline using the Gemini 2.0 Flash model to convert full-text documents into structured Markdown inputs given a simple prompt (Supplementary Methods), which was then processed by GPT 4.1 for full-text screening. Full-text PDF articles were programmatically retrieved through OpenAlex37, a comprehensive corpus of over 240 million scholarly works sourced from open platforms such as Crossref, MAG, DataCite, HAL, PubMed, Institutional Repositories. Articles that were not available through OpenAlex but were included after abstract screening were retrieved via institutional access.

In all instances, GPT-4.1 was prompted using the original, unaltered objectives and eligibility criteria from each respective review (Supplementary Notes).

8.3 Article Screening Benchmarking
We evaluated screening performance using a diagnostic test accuracy (DTA) study design. Our reference standard was the final article inclusion or exclusion decisions of the original review authors after full-text screening. “Included” articles represented the final set of articles included in each review, and “excluded” articles represented articles excluded from title, abstract, and full-text screening in each review.

We tested the otto-SR screening agent as a standalone reviewer across the full set of citations retrieved in each SR (n = 32,357) (Extended Data Table 3, 4). Screening was conducted in two stages: first, an abstract-level screen was applied to all citations; those marked as “included” then underwent full-text screening to determine the final set of included articles.

For comparison, we assessed the performance of Elicit (evaluated on April 12, 2025), a commercially available systematic review automation software, and a panel of human reviewers against a representative sample of citations from each SR. To assess sensitivity, we included all articles deemed ‘included’ in each SR (i.e., entire inclusion set) where possible. For specificity, we determined a minimum specificity sample size of 139 ‘excluded’ articles with Cochran’s sample size38, based on an expected specificity of 90%, 5% margin of error, and 95% confidence level. These excluded articles were randomly sampled from each review’s pool of non-included citations (Extended Data Table 3, 4).

To evaluate the performance of Elicit in screening, we uploaded all PDF articles for each sample into the platform. Elicit was tested on a sample of citations, rather than the full dataset, due to its 500-record screening limit per review. The inclusion criteria provided to Elicit was identical to the inclusion criteria provided to the otto-SR screening agent. As Elicit does not natively support exclusion criteria, we tested both inclusion criteria alone and inclusion criteria combined with inverse exclusion terms. Performance was higher using inclusion criteria alone (Extended Data Table 1), so this approach was adopted. Elicit automatically retrieved titles and abstracts, followed by screening using a default inclusion score threshold of 2.5. No full-text screening workflow was available. All evaluations were conducted using Elicit’s paid “Pro” plan.

To evaluate the performance of humans in data extraction, we assembled a panel of four graduate-level researchers with past SR experience (1 BSc, 3 MSc; all current MD students) to perform dual screening9. All screening was performed independently and in duplicate. Conflicts during screening were resolved by a third independent reviewer. Screening followed a standard end-to-end workflow: all citations were screened at the title/abstract stage, and citations deemed eligible by reviewers were advanced to full-text screening.

8.4 Human Calibration
To verify screening proficiency, human reviewers first completed a calibration exercise using a set of citations from SeroTracker, a comprehensive systematic review on SARS-CoV-232,39. In SeroTracker, the original study authors conducted a study to assess internal consistency through re-screening a previously screened dataset using the same eligibility criteria12. Our reviewers screened this same dataset, and we compared their performance against the original SeroTracker authors’ results.The performance of our reviewers was comparable to the original SeroTracker authors. Details are found in Extended Data Table 2.

8.5 Screening Data Analysis
We assessed the performance of the otto-SR screening agent, Elicit, and dual human reviewers by analyzing accuracy, sensitivity, specificity; and reported true positives, true negatives, false positives, and false negatives. We calculated 95% CIs for weighted (pooled-denominator) sensitivity and specificity using the Wilson method40 with the binom package in R.

8.6 Data Extraction Datasets
We utilized four datasets (SeroTracker, PA-Outcomes, PA-Testing, Sepsis) from the BenchSR database that contained raw data extraction results provided by the original authors. In addition, we identified three external SRs (CKD41, Process42, Psyc-meds43) that also provided publicly accessible raw data extraction information (Extended Data Table 5). Variables assessed for extraction included key descriptive and outcome data used by the original authors for downstream analysis (see Supplementary Notes).

8.7 LLM Data Extraction Methodology
We developed a novel LLM-based data extraction agent using prompting best practices. Our data extraction agent uses the o3-mini-high model from OpenAI, high reasoning effort and a 100,000-token output limit. The same markdown extracted from full-article PDF as used in the full-text screens were passed as inputs to o3-mini-high for extraction. In all cases, the extraction agent was prompted with author-defined variables and corresponding descriptions (Supplementary Notes).

8.8 Data Extraction Benchmarking
We evaluated the performance of the otto-SR data extraction agent, Elicit, and dual human reviewers for data extraction across all datasets. The variable definitions used for extraction are provided in the Supplementary Notes.

For the otto-SR data extraction agent and Elicit, we used the complete set of articles with available data extraction results. Due to its extensive size (n=2,736 included articles), the SeroTracker dataset was randomly downsampled to 100 articles for evaluation. For the Psyc-meds dataset, only studies with published data were included (Extended Data Table 5).

To evaluate the performance of Elicit in data extraction (evaluated on March 22, 2025), we uploaded all articles into the Elicit data extraction platform and used the same variable descriptions provided to the otto-SR extraction agent (Supplementary notes). All extractions were performed with the ‘high accuracy’ feature, accessed through the Elicit ‘Pro’ paid plan. In cases where Elicit encountered an error or failed to extract data, we retried up to a maximum of 5 times.

To evaluate the performance of humans in screening, we assembled a panel of seven graduate-level researchers with past SR experience (3 BSc, 4 MSc; all current MD students). Human data extraction was performed independently and in duplicate. Discrepancies were resolved by a third human reviewer. For human data extraction, we determined sample size using a McNemar test for sample size approximation. Using an estimated human accuracy of 80% (reported rates: 65.8-85.5%)14–19, LLM accuracy of 90%, and 95% confidence, we determined a minimum number of 204 variables per study to be extracted. Article counts for each testing dataset are provided in Extended Data Table 5.

Due to the unstandardized nature of data extraction results (e.g., SeroTracker review - name of immunoassay used), we used an LLM-as-a-judge framework to programmatically determine data extraction accuracy. In this setup, the o3-mini-high LLM was used to compare each AI-or human-extracted value to the original author value and determine if the two were equivalent. This evaluation method has been validated in prior LLM benchmarking efforts, including the LLM Chatbot arena26 and OpenAI’s HealthBench27.

8.9 Data Extraction Correction
Prior research has shown wide variability in the accuracy of dual human extraction, with reported rates ranging from 65.8-85.5%14–19. As such, original author-provided values did not represent a reliable ground truth. To address this, we conducted a blinded correction process for cases where the LLM-as-a-judge flagged discrepancies between otto-SR extracted values and original review author extracted values. A panel of three independent, experienced graduate-level human reviewers validated outputs. Reviewers were presented with two anonymized and randomized responses (LLM and original author) and asked to select one of four options: Option A correct, Option B correct, Both correct, or Neither correct. Each disagreement was evaluated in parallel and resolved by a third independent arbitrator. The final adjudicated results were used to construct corrected ground truth datasets. To evaluate the accuracy of otto-SR, Elicit, and dual human reviewers, we then applied the LLM-as-a-judge framework to compare each system’s outputs against this corrected dataset. This adjudication framework was adapted from established protocols in radiology, ophthalmology, and clinical trials28–31.

We note a potential limitation in our validation process: when otto-SR and the original authors produced identical values, we assumed these were correct without further adjudication. Consequently, if both sources made the same systematic error, it would go undetected. This approach could potentially bias our evaluation against alternative models (e.g., Elicit or dual human reviewers) that disagreed with both reference sources. To evaluate this limitation, we performed spot checks on a random 10% sample of extractions where otto-SR and the original authors agreed, finding no errors or inconsistencies.

We additionally performed a post-hoc review of incorrect AI outputs to classify errors as either ‘parsing errors’ (i.e., errors with the PDF parsing pipeline), ‘inaccessible’ (i.e., data accessible only through author correspondence or supplementary material), or ‘true errors’ (i.e., cases where the original author values were correctly extracted).

8.10 Extraction Data Analysis
We assessed the performance of the otto-SR data extraction agent, Elicit, and human reviewers by analyzing the total accuracy at a variable level per study. If the human adjudicator classification was “inaccessible”, the data point was removed from analysis for otto-SR, Elicit, and human reviewers. We calculated 95% CIs for weighted (pooled-denominator) accuracy using the Wilson method40 with the binom package in R.

8.11 Cochrane Reproducibility
To evaluate the reliability and generalizability of our automated systematic review workflow, we conducted a focused reproducibility assessment using an entire issue of the Cochrane Database of Systematic Reviews. Our aim was to approximate each review’s workflow end-to-end, from literature search through to data extraction and meta-analysis, using the otto-SR pipeline.

We selected the April 2024 issue through random sampling. Of the 14 reviews published, two were excluded: one due to the absence of downloadable data, and another due to an irreproducible search strategy (authors provided a search strategy to populate the Cochrane specialized register, but not for the review itself). This left 12 eligible reviews spanning a range of clinical domains (Extended Data Table 7). The Cochrane database was chosen for its rigorous and standardized reporting practices, public data availability, and detailed methodological documentation.

8.12 Cochrane Database Searches
The original search strategy of each Cochrane review was reproduced using the exact terms and filters described in the review methods. Searches were limited to institutionally accessible databases. In cases where databases lacked precise date filtering (e.g., supporting month but not day-level granularity), we applied post-hoc filtering to approximate the original search window (Supplementary Data 2).

After each search, we cross-referenced our list of articles with those included in the original Cochrane reviews. Articles that were not retrievable from the original search were excluded from downstream screening, data extraction and analysis.

8.13 Cochrane Screening
All retrieved citations underwent abstract and full-text screening with the otto-SR screening agent, prompted with the inclusion and exclusion criteria, objectives, and review protocols from each Cochrane review (Supplementary Notes).

To ensure a focused and interpretable comparison, we deviated from Cochrane’s inclusion practice in one key respect. Cochrane reviews typically include all studies reporting the eligible population and intervention of interest. This approach allows authors to explore all comparisons based on the available data (e.g., all intervention and outcome combinations, including those not pre-specified). While valuable for comprehensive synthesis, the generation of comparisons after screening can make study eligibility unclear.

Instead of focusing on all possible comparisons, we focused our analysis to reproduce each Cochrane review’s primary analytical comparison. This constraint allowed for unambiguous inclusion criteria, where studies had to meet specific predefined interventions, comparators, and outcome criteria. Citations without a retrievable abstract or DOI/trial identifier were excluded. Screening decisions were compared against Cochrane author decisions to calculate true positives, false negatives, false positives, and true negatives.

8.14 Cochrane Data Extraction
For all studies passing full-text screening, outcome data was extracted using the otto-SR extraction agent, focusing exclusively on the primary outcome defined in each Cochrane review. To ensure consistency, we used the original author-defined variable names and extraction logic (Supplementary Notes).

Data extraction also served as a secondary filter. While otto-SR achieved high specificity (∼97%), for a review of 10,000 citations, this would equate to nearly 300 false positive articles. To counteract this, studies were programmatically excluded if they returned unavailable or unreportable values for the primary outcome (e.g., “na” values), were identified as duplicates, or involved ineligible intervention-comparator pairs (e.g., Drug A vs. Drug B when only Drug A vs. placebo was eligible). This secondary filtering step helped remove residual false positives from the screening phase, though it may have introduced occasional misclassifications (then labeled as false negatives).

8.15 Analysis
All meta-analyses were conducted using the metafor package in R (Code and Dataset Availability). To ensure fair comparison, we matched the original authors’ specified meta-analytic model (random-effects, fixed-effect), effect size metric (risk ratio, odds ratio, rate ratio, mean difference, standardized mean difference), and continuity correction approach, where reported. We conducted four meta-analytical comparisons: (1) Cochrane – meta-analysis using the original author-extracted data. (2) Matched – otto-SR results filtered to match the Cochrane primary analysis study set. (3) Expanded – all eligible studies included by otto-SR under the original search cut-off. (4) Updated – all eligible studies included by otto-SR from an updated search extending to May 8 2025. We also derived ‘corrected’ values (see below), for the ‘matched,’ ‘expanded,’ and ‘updated’ comparisons that served as the reference ground truth for analytical comparison.

8.16 Cochrane Data Correction and Comparison
To address known concerns about the reliability of original author-extracted data, we conducted an adjudication process to derive corrected data extraction and screening information for the ‘Matched,’ ‘Expanded,’ and ‘Updated’ analyses. For our ‘Matched’ analysis, a panel of two human reviewers compared data extraction and screening decisions from the original Cochrane authors and the otto-SR extraction agent, selecting the correct value through re-assessment of the source article. In our ‘Expanded’ and ‘Updated’ analysis, where Cochrane data was not available, a panel of two human reviewers compared otto-SR data extraction and screening decisions against original study articles, selecting the correct value through re-assessment of the source article. Final articles included in the corrected analysis consisted of all otto-SR true positive articles, and any Cochrane true positive articles missed by otto-SR. The extracted values in this final dataset reflected the most accurate, reviewer-verified information and served as the reference standard for Cochrane and otto-SR performance comparisons (Supplementary Data 1 for raw and corrected values, reviewer notes, and error classifications).

To ensure consistency and transparency, we applied standardized rules for study eligibility across analysis sets. First, articles had to be retrievable through our reproduced search; unretrievable citations were excluded from all otto-SR-based analyses. Second, for author data requests, we included studies in the Cochrane analysis only if authors explicitly stated that they contacted study authors and specified which studies and outcomes were supplemented. If a data request was suspected for a study, but the review only reported vague references to data requests without further specification, the study was considered unverifiable and excluded from both the Cochrane and otto-SR-corrected analyses. For instance, in the ACEi review, the authors appeared to assign zero mortality events in studies that did not report mortality or adverse events; but did not clearly state which studies had data requests. For such cases, we excluded those studies from the Cochrane and otto-SR corrected analyses to avoid introducing speculative data. Suspected data requests occurred in three reviews (4 studies, Nutrition; 15 studies, ACEi; 4 studies, Depression). A high-level summary of methodological issues is provided in Extended Data Table 7. Detailed notes for the exclusion of studies are provided in Supplementary Data 1.

Studies with supplementary data (not extractable by otto-SR) were included in the Cochrane and corrected analyses, thereby penalizing the model. If the data was inaccessible to otto-SR due to format limitations (e.g., embedded figures), the study was excluded from otto-SR analyses but retained in the Cochrane and corrected sets. These criteria aimed to balance reproducibility, verifiability, and the practical constraints of automation.

Data Availability
All datasets and code used for data analysis will be made available on publication.

10 Author Information
These authors contributed equally: C.C, R.A., P.C.

Author contribution statements: C.C, R.A, N.B contributed to the conception and design of the work. C.C, R.A, K.M, M.C, E.F, A.S, R.K, R.S, D.M, J.L, J.J, D.C, J.G, S.L contributed to data acquisition, cleaning, human comparisons, and human arbitration. C.C, R.A, P.C, J.S, S.J, J.X, K.Z generated code for evaluations and benchmarking. C.C, R.A, P.C, L.X.G analyzed study data. N.B, G.M.C, D.M, I.B, D.B.E, R.K.A, L.G, M.N, A.A.L, B.T, M.W, H.W contributed to project supervision and provided feedback on the study. C.C, R.A, P.C, prepared the original draft of the manuscript with input from all co-authors. All authors were responsible for review and editing of the manuscript. All authors debated, discussed, edited, and approved the final version of the manuscript.

11 Ethics Declaration
There was no direct funding support for this manuscript.

N.B reports grants from the Public Health Agency of Canada through Canada’s COVID-19 Immunity Task Force, the World Health Organization Health Emergencies Programme, the Robert Koch Institute, and the Canadian Medical Association Joule Innovation Fund, the Canadian Association of Emergency Physicians and Alberta Health Services Emergency Strategic Clinical Network.

Disclosures for G.M.C. can be found at http://arep.med.harvard.edu/gmc/tech.html.

R.K.A. is employed at OpenAI and owns stock as part of the standard compensation package.

R.A. reports grants from the CIHR Institute of Genetics.

C.C. P.C. J.S. are founders of and hold equity in otto review, LLC. R.A is a non equity holding founder/advisor in otto review, LLC.

No funding source had any role in the design of this study, its execution, analyses, interpretation of the data, or decision to submit results.

12 Code and Dataset Availability
All datasets and code used for data analysis will be made available on publication.
